{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a896b2ab",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b97a16-b790-4e4c-b0fa-fc0212fbcfe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as n\n",
    "import os\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, average_precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd1370-0597-42df-aeac-61e4e0a03fb7",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad917d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory where the folds are saved\n",
    "folds_dir = '/path/to/folds'\n",
    "\n",
    "# Initialize lists to store data for each fold\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# Load train and test folds for each fold\n",
    "for fold in range(1, 6):\n",
    "    train_file = os.path.join(folds_dir, f'fold_{fold}_train.csv')\n",
    "    test_file = os.path.join(folds_dir, f'fold_{fold}_test.csv')\n",
    "    \n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    \n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "    \n",
    "    print(f\"Fold {fold} - Train data shape: {train_data.shape}\")\n",
    "    print(f\"Fold {fold} - Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Access individual fold data using train_data_list[fold_index] and test_data_list[fold_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945de63-f2c6-4d85-92c4-4858dc4e9b79",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f58a618-d3e7-4b6c-b589-ac19b7be6da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0634c57-49d7-4632-8f2c-de3b5c9f5113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess text data.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Re-join tokens into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e9b631-d2c5-4e52-9947-eb24c5d0ef56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing to each train and test fold\n",
    "for i in range(len(train_data_list)):\n",
    "    train_data_list[i]['Text_desc'] = train_data_list[i]['Text_desc'].apply(preprocess_text)\n",
    "    test_data_list[i]['Text_desc'] = test_data_list[i]['Text_desc'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38ae41-f56f-4ac0-bb8f-77fb2108dbcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b5c838-4107-4f82-a155-228ab0b382b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_classifier(X_train, y_train, C, regularisation):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression classifier for multi-label classification.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Features for training\n",
    "    - y_train: Labels for training\n",
    "    - C: Regularization strength\n",
    "    - regularisation: Type of regularization ('l1' or 'l2')\n",
    "\n",
    "    Returns:\n",
    "    - Trained MultiOutputClassifier model\n",
    "    \"\"\"\n",
    "    model = MultiOutputClassifier(LogisticRegression(penalty=regularisation, C=C, class_weight='balanced', max_iter=10000))\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5e30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import IterativeStratification\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store probability logits\n",
    "probability_logits_bow = []\n",
    "probability_logits_tfidf = []\n",
    "\n",
    "# New directory to save the DataFrames\n",
    "new_dir = '/Users/jeremybalch/Desktop/BoW-TFIDF_logits_v10'\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "# Use the predefined train and test splits\n",
    "for fold_index, (train_data, test_data) in enumerate(zip(train_data_list, test_data_list)):\n",
    "    X_train = train_data['Text_desc']\n",
    "    y_train = train_data.drop(columns=['Text_desc'])\n",
    "    X_test = test_data['Text_desc']\n",
    "    y_test = test_data.drop(columns=['Text_desc'])\n",
    "    \n",
    "    # Initialize CountVectorizer and TfidfVectorizer\n",
    "    bow_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    \n",
    "    # Fit and transform train data\n",
    "    X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test_bow = bow_vectorizer.transform(X_test)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train classifiers\n",
    "    classifier_mybag = train_classifier(X_train_bow, y_train, C=1.0, regularisation='l2')\n",
    "    classifier_tfidf = train_classifier(X_train_tfidf, y_train, C=10, regularisation='l2')\n",
    "    \n",
    "    # Get probability logits for the positive class (class 1)\n",
    "    y_test_predicted_scores_mybag = np.hstack([proba[:, 1].reshape(-1, 1) for proba in classifier_mybag.predict_proba(X_test_bow)])\n",
    "    y_test_predicted_scores_tfidf = np.hstack([proba[:, 1].reshape(-1, 1) for proba in classifier_tfidf.predict_proba(X_test_tfidf)])\n",
    "    \n",
    "    # Debug: Print shapes\n",
    "    print(\"Shape of y_test_predicted_scores_mybag:\", y_test_predicted_scores_mybag.shape)\n",
    "    print(\"Shape of y_test_predicted_scores_tfidf:\", y_test_predicted_scores_tfidf.shape)\n",
    "    \n",
    "    # Ensure the predicted scores have the correct number of classes\n",
    "    if y_test_predicted_scores_mybag.shape[1] != 21 or y_test_predicted_scores_tfidf.shape[1] != 21:\n",
    "        raise ValueError(\"Predicted scores do not match the expected number of classes (21).\")\n",
    "\n",
    "    # Save the probability logits for each fold\n",
    "    df_bow_fold = pd.DataFrame(y_test_predicted_scores_mybag)\n",
    "    df_tfidf_fold = pd.DataFrame(y_test_predicted_scores_tfidf)\n",
    "    \n",
    "    df_bow_fold.to_csv(f'{new_dir}/probability_logits_bow_fold_{fold_index}.csv', index=False)\n",
    "    df_tfidf_fold.to_csv(f'{new_dir}/probability_logits_tfidf_fold_{fold_index}.csv', index=False)\n",
    "    \n",
    "    # Save the true labels for each fold\n",
    "    df_y_test = pd.DataFrame(y_test)\n",
    "    df_y_test.to_csv(f'{new_dir}/true_labels_fold_{fold_index}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebdc38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fold  Label  F1_Score_BoW  F1_Score_TFIDF\n",
      "0       0      0      0.222222        0.082192\n",
      "1       0      1      0.571429        0.128205\n",
      "2       0      2      0.745763        0.474227\n",
      "3       0      3      0.545455        0.520000\n",
      "4       0      4      0.652632        0.666667\n",
      "..    ...    ...           ...             ...\n",
      "100     4     16      0.773333        0.558559\n",
      "101     4     17      0.181818        0.117647\n",
      "102     4     18      0.740741        0.608696\n",
      "103     4     19      0.818182        0.241758\n",
      "104     4     20      0.461538        0.098765\n",
      "\n",
      "[105 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store F1 scores for each label and fold\n",
    "f1_scores_per_label = []\n",
    "\n",
    "# Perform the split again to access y_test for each fold\n",
    "for fold_index in range(5):  # Assuming 5 folds\n",
    "    # Load the saved logits\n",
    "    df_bow_fold = pd.read_csv(f'path/to/probability_logits_bow_fold_{fold_index}.csv')\n",
    "    df_tfidf_fold = pd.read_csv(f'/path/to/probability_logits_tfidf_fold_{fold_index}.csv')\n",
    "    \n",
    "    # Load the true labels for the current fold\n",
    "    df_y_test = pd.read_csv(f'path/to/true_labels_fold_{fold_index}.csv')\n",
    "    y_test = df_y_test.values\n",
    "    \n",
    "    # Convert logits to binary predictions using a threshold of 0.5\n",
    "    y_test_pred_mybag = (df_bow_fold.values >= 0.11).astype(int)\n",
    "    y_test_pred_tfidf = (df_tfidf_fold.values >= 0.11).astype(int)\n",
    "    \n",
    "    # Calculate F1 scores for each label\n",
    "    f1_scores_mybag = f1_score(y_test, y_test_pred_mybag, average=None)\n",
    "    f1_scores_tfidf = f1_score(y_test, y_test_pred_tfidf, average=None)\n",
    "    \n",
    "    # Store F1 scores in a DataFrame\n",
    "    df_f1_scores = pd.DataFrame({\n",
    "        'Fold': fold_index,\n",
    "        'Label': range(y_test.shape[1]),\n",
    "        'F1_Score_BoW': f1_scores_mybag,\n",
    "        'F1_Score_TFIDF': f1_scores_tfidf\n",
    "    })\n",
    "    \n",
    "    f1_scores_per_label.append(df_f1_scores)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "f1_scores_df = pd.concat(f1_scores_per_label, ignore_index=True)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "f1_scores_df.to_csv('path/to/f1_scores_per_label.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(f1_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf307844",
   "metadata": {},
   "source": [
    "### Get min, max, and avg F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `y_cols` is defined as follows:\n",
    "# y_cols = list(data_clean.columns[1:])\n",
    "\n",
    "# Calculate min, max, and average F1 scores for each label\n",
    "f1_summary = f1_scores_df.groupby('Label').agg(\n",
    "    Min_F1_Score_BoW=('F1_Score_BoW', 'min'),\n",
    "    Max_F1_Score_BoW=('F1_Score_BoW', 'max'),\n",
    "    Avg_F1_Score_BoW=('F1_Score_BoW', 'mean'),\n",
    "    Min_F1_Score_TFIDF=('F1_Score_TFIDF', 'min'),\n",
    "    Max_F1_Score_TFIDF=('F1_Score_TFIDF', 'max'),\n",
    "    Avg_F1_Score_TFIDF=('F1_Score_TFIDF', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Map label indices to names using y_cols\n",
    "f1_summary['Label_Name'] = f1_summary['Label'].map(lambda x: y_cols[x])\n",
    "\n",
    "# Reorder columns to have Label_Name first\n",
    "f1_summary = f1_summary[['Label_Name', 'Label', 'Min_F1_Score_BoW', 'Max_F1_Score_BoW', 'Avg_F1_Score_BoW', \n",
    "                         'Min_F1_Score_TFIDF', 'Max_F1_Score_TFIDF', 'Avg_F1_Score_TFIDF']]\n",
    "\n",
    "# Optionally, save the summary DataFrame to a CSV file\n",
    "f1_summary.to_csv('path/to/f1_summary_per_label.csv', index=False)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(f1_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc90089",
   "metadata": {},
   "source": [
    "### Hamming Loss Across All Labels for Each Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02ab756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss for BoW: 0.1391561806069578\n",
      "Hamming Loss for TFIDF: 0.7510486059708857\n",
      "Hamming Loss for BoW (Fold 0): 0.16216216216216217\n",
      "Hamming Loss for TFIDF (Fold 0): 0.7458172458172458\n",
      "Hamming Loss for BoW (Fold 1): 0.12825396825396826\n",
      "Hamming Loss for TFIDF (Fold 1): 0.746031746031746\n",
      "Hamming Loss for BoW (Fold 2): 0.1391941391941392\n",
      "Hamming Loss for TFIDF (Fold 2): 0.7411477411477412\n",
      "Hamming Loss for BoW (Fold 3): 0.12839059674502712\n",
      "Hamming Loss for TFIDF (Fold 3): 0.7631103074141049\n",
      "Hamming Loss for BoW (Fold 4): 0.1386904761904762\n",
      "Hamming Loss for TFIDF (Fold 4): 0.7583333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store true labels and predictions for the whole dataset\n",
    "all_true_labels = []\n",
    "all_pred_labels_mybag = []\n",
    "all_pred_labels_tfidf = []\n",
    "\n",
    "# Initialize lists to store Hamming loss for each fold\n",
    "hamming_loss_per_fold_mybag = []\n",
    "hamming_loss_per_fold_tfidf = []\n",
    "\n",
    "# Perform the split again to access y_test for each fold\n",
    "for fold_index in range(5):  # Assuming 5 folds\n",
    "    # Load the saved logits\n",
    "    df_bow_fold = pd.read_csv(f'path/to/probability_logits_bow_fold_{fold_index}.csv')\n",
    "    df_tfidf_fold = pd.read_csv(f'path/to/probability_logits_tfidf_fold_{fold_index}.csv')\n",
    "    \n",
    "    # Load the true labels for the current fold\n",
    "    df_y_test = pd.read_csv(f'path/to/true_labels_fold_{fold_index}.csv')\n",
    "    y_test = df_y_test.values\n",
    "    \n",
    "    # Convert logits to binary predictions using a threshold of 0.15\n",
    "    y_test_pred_mybag = (df_bow_fold.values >= 0.15).astype(int)\n",
    "    y_test_pred_tfidf = (df_tfidf_fold.values >= 0.15).astype(int)\n",
    "    \n",
    "    # Append true labels and predictions to the lists\n",
    "    all_true_labels.append(y_test)\n",
    "    all_pred_labels_mybag.append(y_test_pred_mybag)\n",
    "    all_pred_labels_tfidf.append(y_test_pred_tfidf)\n",
    "    \n",
    "    # Calculate Hamming loss for the current fold\n",
    "    hamming_loss_mybag = hamming_loss(y_test, y_test_pred_mybag)\n",
    "    hamming_loss_tfidf = hamming_loss(y_test, y_test_pred_tfidf)\n",
    "    \n",
    "    # Append Hamming loss for the current fold to the lists\n",
    "    hamming_loss_per_fold_mybag.append(hamming_loss_mybag)\n",
    "    hamming_loss_per_fold_tfidf.append(hamming_loss_tfidf)\n",
    "\n",
    "# Concatenate all folds to get the complete dataset\n",
    "all_true_labels = np.vstack(all_true_labels)\n",
    "all_pred_labels_mybag = np.vstack(all_pred_labels_mybag)\n",
    "all_pred_labels_tfidf = np.vstack(all_pred_labels_tfidf)\n",
    "\n",
    "# Calculate Hamming loss for the whole dataset\n",
    "hamming_loss_mybag = hamming_loss(all_true_labels, all_pred_labels_mybag)\n",
    "hamming_loss_tfidf = hamming_loss(all_true_labels, all_pred_labels_tfidf)\n",
    "\n",
    "print(f\"Hamming Loss for BoW: {hamming_loss_mybag}\")\n",
    "print(f\"Hamming Loss for TFIDF: {hamming_loss_tfidf}\")\n",
    "\n",
    "# Print Hamming loss for each fold\n",
    "for fold_index in range(5):\n",
    "    print(f\"Hamming Loss for BoW (Fold {fold_index}): {hamming_loss_per_fold_mybag[fold_index]}\")\n",
    "    print(f\"Hamming Loss for TFIDF (Fold {fold_index}): {hamming_loss_per_fold_tfidf[fold_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cccaee",
   "metadata": {},
   "source": [
    "### All metrics for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store metrics for each label and fold\n",
    "metrics_per_label = []\n",
    "\n",
    "# Perform the split again to access y_test for each fold\n",
    "for fold_index in range(5):  # Assuming 5 folds\n",
    "    # Load the saved logits\n",
    "    df_bow_fold = pd.read_csv(f'path/to0/probability_logits_bow_fold_{fold_index}.csv')\n",
    "    df_tfidf_fold = pd.read_csv(f'path/to/probability_logits_tfidf_fold_{fold_index}.csv')\n",
    "    \n",
    "    # Load the true labels for the current fold\n",
    "    df_y_test = pd.read_csv(f'path/to/true_labels_fold_{fold_index}.csv')\n",
    "    y_test = df_y_test.values\n",
    "    \n",
    "    # Convert logits to binary predictions using a threshold of 0.3\n",
    "    y_test_pred_mybag = (df_bow_fold.values >= 0.3).astype(int)\n",
    "    y_test_pred_tfidf = (df_tfidf_fold.values >= 0.3).astype(int)\n",
    "    \n",
    "    # Calculate metrics for each label\n",
    "    for label_index in range(y_test.shape[1]):\n",
    "        # Initialize metrics\n",
    "        auroc_mybag = None\n",
    "        auroc_tfidf = None\n",
    "        \n",
    "        # Calculate AUROC and AUPRC\n",
    "        try:\n",
    "            auroc_mybag = roc_auc_score(y_test[:, label_index], df_bow_fold.values[:, label_index])\n",
    "        except ValueError:\n",
    "            pass  # Skip AUROC calculation if only one class is present\n",
    "        \n",
    "        auprc_mybag = average_precision_score(y_test[:, label_index], df_bow_fold.values[:, label_index])\n",
    "        \n",
    "        try:\n",
    "            auroc_tfidf = roc_auc_score(y_test[:, label_index], df_tfidf_fold.values[:, label_index])\n",
    "        except ValueError:\n",
    "            pass  # Skip AUROC calculation if only one class is present\n",
    "        \n",
    "        auprc_tfidf = average_precision_score(y_test[:, label_index], df_tfidf_fold.values[:, label_index])\n",
    "        \n",
    "        # Calculate accuracy and F1\n",
    "        accuracy_mybag = accuracy_score(y_test[:, label_index], y_test_pred_mybag[:, label_index])\n",
    "        f1_mybag = f1_score(y_test[:, label_index], y_test_pred_mybag[:, label_index])\n",
    "        \n",
    "        accuracy_tfidf = accuracy_score(y_test[:, label_index], y_test_pred_tfidf[:, label_index])\n",
    "        f1_tfidf = f1_score(y_test[:, label_index], y_test_pred_tfidf[:, label_index])\n",
    "        \n",
    "        # Store metrics in a DataFrame\n",
    "        metrics_per_label.append({\n",
    "            'Label': label_index,\n",
    "            'AUROC_BoW': auroc_mybag,\n",
    "            'AUPRC_BoW': auprc_mybag,\n",
    "            'Accuracy_BoW': accuracy_mybag,\n",
    "            'F1_BoW': f1_mybag,\n",
    "            'AUROC_TFIDF': auroc_tfidf,\n",
    "            'AUPRC_TFIDF': auprc_tfidf,\n",
    "            'Accuracy_TFIDF': accuracy_tfidf,\n",
    "            'F1_TFIDF': f1_tfidf\n",
    "        })\n",
    "\n",
    "# Convert the list of metrics to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_per_label)\n",
    "\n",
    "# Calculate the average metrics for each label\n",
    "average_metrics_df = metrics_df.groupby('Label').mean().reset_index()\n",
    "\n",
    "# Optionally, save the average metrics DataFrame to a CSV file\n",
    "average_metrics_df.to_csv('/Users/jeremybalch/Desktop/BoW-TFIDF_logits_v10/BoW_TFIDF_metrics_per_label.csv', index=False)\n",
    "\n",
    "# Display the average metrics DataFrame\n",
    "print(average_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
