{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34200766",
   "metadata": {},
   "source": [
    "### Clinical Longformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1047735-a846-4e41-b8d1-9f1aa8e04477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05be0bfd-9edb-4cf7-8201-9026d1118104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "CUDA device name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0190bf2",
   "metadata": {},
   "source": [
    "### Load train and test folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2b92d-f9a9-428f-9dfa-6100edbf6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory where the folds are saved\n",
    "folds_dir = 'path/to/folds'\n",
    "\n",
    "# Initialize lists to store data for each fold\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# Load train and test folds for each fold\n",
    "for fold in range(1, 6):\n",
    "    train_file = os.path.join(folds_dir, f'fold_{fold}_train.csv')\n",
    "    test_file = os.path.join(folds_dir, f'fold_{fold}_test.csv')\n",
    "    \n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    \n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "    \n",
    "    print(f\"Fold {fold} - Train data shape: {train_data.shape}\")\n",
    "    print(f\"Fold {fold} - Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Access individual fold data using train_data_list[fold_index] and test_data_list[fold_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e3ef4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['Active_bleeding_from_named_vessel', 'Active_bleeding_from_solid_organ',\n",
    "               'Bowel_resection', 'Class_I', 'Class_II', 'Class_III', \n",
    "               'Class_IV', 'Colostomy', 'Fascia_Closed_(Interrupted)', \n",
    "               'Fascia_Closed_(Running/Continuous)', \n",
    "               'Fascia_Left_Open', 'Hand-Sewn_Anastomosis', \n",
    "               'Ileostomy', 'Primary_Repair', \n",
    "               'Serosal_tear_repair', 'Skin_Closed_(Full w/ Prevena)', \n",
    "               'Skin_Closed_(Full)', 'Skin_Closed_(Partial)', \n",
    "               'Skin_Left_Open', 'Stapled_Anastomosis', 'Synthetic']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ef466-5f9b-4c07-a523-16eeb8c7e507",
   "metadata": {},
   "source": [
    "### Load Model, Tokenizer, and Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e86bb25-5d1f-42f0-bbda-e599ab4f4aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad08c36-f88a-4848-beb7-2fa47e791257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yikuan8/Clinical-Longformer\", num_labels=len(label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1da60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(\n",
    "    train_data['Text_desc'].tolist(),\n",
    "    truncation='only_second',  # tokenizing the latter part when token length is exceeded \n",
    "    max_length=4096,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_data['Text_desc'].tolist(),\n",
    "    truncation='only_second',  # tokenizing the latter part when token length is exceeded \n",
    "    max_length=4096,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dde4dc",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67e4ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_data)):\n",
    "    train_fold_encodings = tokenizer(\n",
    "        train_data.iloc[train_index]['Text_desc'].tolist(),\n",
    "        truncation='only_second',\n",
    "        max_length=4096,\n",
    "        padding=True\n",
    "    )\n",
    "    test_fold_encodings = tokenizer(\n",
    "        train_data.iloc[test_index]['Text_desc'].tolist(),\n",
    "        truncation='only_second',\n",
    "        max_length=4096,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    train_fold_dataset = ReadDataset(train_fold_encodings, train_data.iloc[train_index][label_cols].values)\n",
    "    test_fold_dataset = ReadDataset(test_fold_encodings, train_data.iloc[test_index][label_cols].values)\n",
    "    \n",
    "    train_datasets.append(train_fold_dataset)\n",
    "    test_datasets.append(test_fold_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7959dda",
   "metadata": {},
   "source": [
    "### Calculate Class Weights and Build Custome Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6b3ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# Calculate class weights\n",
    "original_path = '/path/to/op_note_data.csv' \n",
    "data = pd.read_csv(original_path)\n",
    "data = data.drop(['ID', 'Unnamed: 0', 'wound_class'], axis=1) \n",
    "\n",
    "class_counts = data[label_cols].sum()\n",
    "total_counts = len(data)\n",
    "class_weights = total_counts / (len(label_cols) * class_counts)\n",
    "class_weights = torch.tensor(class_weights.values, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
    "                        labels.float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5098e",
   "metadata": {},
   "source": [
    "### Function to compute metrics for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de8a7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return 1 / (1 + np.exp(-x))  # Correct sigmoid function\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = sigmoid(logits) > 0.1  # Convert logits to binary predictions with a threshold of 0.5\n",
    "    \n",
    "    # Debugging: Print some logits, predictions, and labels\n",
    "    print(\"Logits:\", logits[:5])\n",
    "    print(\"Predictions:\", preds[:5])\n",
    "    print(\"Labels:\", labels[:5])\n",
    "    \n",
    "    res = {}\n",
    "    for i, d in enumerate(label_cols):\n",
    "        res[f'f1_{d}'] = f1_score(labels[:, i], preds[:, i])  # F1 score for each label\n",
    "    res['f1_micro'] = f1_score(labels, preds, average='micro')  # Micro F1 score across all labels\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73170424",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82eccce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, LongformerForSequenceClassification\n",
    "\n",
    "output_dir = 'path/to/output_directory'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "logging_dir = 'path/to/logging_directory'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,          # output directory\n",
    "    num_train_epochs=500,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_micro',\n",
    "    greater_is_better=True,\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0f0ae",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ade5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = MultilabelTrainer(\n",
    "    model=model,                         # model, defined above\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691451c2",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6be68",
   "metadata": {},
   "source": [
    "### Find best f1 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a360c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the path to the saved logits\n",
    "logits_paths = [\n",
    "    f'/path/to/logits_fold_{fold}.csv'\n",
    "    for fold in range(1, 6)\n",
    "]\n",
    "\n",
    "all_logits = []\n",
    "\n",
    "# Load the logits\n",
    "for logits_path in logits_paths:\n",
    "    logits = np.loadtxt(logits_path, delimiter=\",\")\n",
    "    all_logits.append(logits)\n",
    "\n",
    "# Find the best threshold for each fold\n",
    "best_thresholds = []\n",
    "\n",
    "for fold, (true_labels, logits) in enumerate(zip(all_true_labels, all_logits), start=1):\n",
    "    best_threshold = 0.0\n",
    "    best_f1_score = 0.0\n",
    "    \n",
    "    for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "        probabilities = sigmoid(logits)\n",
    "        predictions = (probabilities > threshold).astype(int)\n",
    "        \n",
    "        # Calculate average F1 score\n",
    "        avg_f1 = f1_score(true_labels, predictions, average='micro')\n",
    "        \n",
    "        if avg_f1 > best_f1_score:\n",
    "            best_f1_score = avg_f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    best_thresholds.append(best_threshold)\n",
    "    print(f\"Fold {fold} - Best threshold: {best_threshold} with average F1 score: {best_f1_score}\")\n",
    "\n",
    "# Calculate the average of the best thresholds\n",
    "average_best_threshold = np.mean(best_thresholds)\n",
    "print(f\"Average of best thresholds across folds: {average_best_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7bfbee",
   "metadata": {},
   "source": [
    "### Calculate Avg F1 Micro and hamming loss across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc954d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the path to the saved logits\n",
    "logits_paths = [\n",
    "    f'/path/to/logits_fold_{fold}.csv'\n",
    "    for fold in range(1, 6)\n",
    "]\n",
    "\n",
    "# Initialize lists to store true labels and predictions\n",
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "# Load the true labels from the test datasets\n",
    "for fold, test_data in enumerate(test_data_list, start=1):\n",
    "    # Drop the 'Text_desc' column to get the true labels\n",
    "    true_labels = test_data.drop(columns=['Text_desc']).values\n",
    "    all_true_labels.append(true_labels)\n",
    "    \n",
    "    # Save true labels for each fold\n",
    "    true_labels_path = f'/path/to/true_labels_fold_{fold}.csv'\n",
    "    np.savetxt(true_labels_path, true_labels, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "f1_micro_scores = []\n",
    "hamming_losses = []\n",
    "\n",
    "# Load the logits and calculate predictions\n",
    "for fold, logits_path in enumerate(logits_paths, start=1):\n",
    "    logits = np.loadtxt(logits_path, delimiter=\",\")\n",
    "    probabilities = sigmoid(logits)\n",
    "    predictions = (probabilities > 0.11).astype(int)\n",
    "    \n",
    "    # Load true labels for the current fold\n",
    "    true_labels_path = f'/path/to/true_labels_fold_{fold}.csv'\n",
    "    true_labels = np.loadtxt(true_labels_path, delimiter=\",\")\n",
    "    \n",
    "    # Calculate F1 micro score and Hamming loss for the current fold\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    hamming_loss_value = hamming_loss(true_labels, predictions)\n",
    "    \n",
    "    f1_micro_scores.append(f1_micro)\n",
    "    hamming_losses.append(hamming_loss_value)\n",
    "\n",
    "# Print the F1 micro scores and Hamming losses for each fold\n",
    "for fold, (f1_micro, hamming_loss_value) in enumerate(zip(f1_micro_scores, hamming_losses), start=1):\n",
    "    print(f\"Fold {fold} - F1 Micro Score: {f1_micro}, Hamming Loss: {hamming_loss_value}\")\n",
    "    \n",
    "# Calculate min, max, and average F1 micro scores and Hamming losses across all folds\n",
    "min_f1_micro = min(f1_micro_scores)\n",
    "max_f1_micro = max(f1_micro_scores)\n",
    "avg_f1_micro = sum(f1_micro_scores) / len(f1_micro_scores)\n",
    "\n",
    "min_hamming_loss = min(hamming_losses)\n",
    "max_hamming_loss = max(hamming_losses)\n",
    "avg_hamming_loss = sum(hamming_losses) / len(hamming_losses)\n",
    "\n",
    "print(f\"Min F1 Micro Score: {min_f1_micro}\")\n",
    "print(f\"Max F1 Micro Score: {max_f1_micro}\")\n",
    "print(f\"Avg F1 Micro Score: {avg_f1_micro}\")\n",
    "\n",
    "print(f\"Min Hamming Loss: {min_hamming_loss}\")\n",
    "print(f\"Max Hamming Loss: {max_hamming_loss}\")\n",
    "print(f\"Avg Hamming Loss: {avg_hamming_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0381a17f",
   "metadata": {},
   "source": [
    "### Create min, max, avg f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "import pandas as pd\n",
    "\n",
    "f1_scores_per_label = []\n",
    "\n",
    "# Load the logits and calculate predictions\n",
    "for fold, logits_path in enumerate(logits_paths, start=1):\n",
    "    logits = np.loadtxt(logits_path, delimiter=\",\")\n",
    "    probabilities = sigmoid(logits)\n",
    "    predictions = (probabilities > 0.11).astype(int)\n",
    "    \n",
    "    # Load true labels for the current fold\n",
    "    true_labels_path = f'/path/to/true_labels_fold_{fold}.csv'\n",
    "    true_labels = np.loadtxt(true_labels_path, delimiter=\",\")\n",
    "    \n",
    "    # Calculate F1 score for each label\n",
    "    f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "    f1_scores_per_label.append(f1_scores)\n",
    "    \n",
    "    # Calculate Hamming loss for the current fold\n",
    "    hamming_loss_value = hamming_loss(true_labels, predictions)\n",
    "    hamming_losses.append(hamming_loss_value)\n",
    "\n",
    "# Create a DataFrame to store F1 scores and Hamming losses\n",
    "f1_scores_df = pd.DataFrame(f1_scores_per_label, columns=label_cols)\n",
    "\n",
    "# Calculate min, max, and average F1 scores for each label\n",
    "min_f1_scores = f1_scores_df[label_cols].min()\n",
    "max_f1_scores = f1_scores_df[label_cols].max()\n",
    "avg_f1_scores = f1_scores_df[label_cols].mean()\n",
    "\n",
    "# Create a new DataFrame to store these statistics\n",
    "f1_stats_df = pd.DataFrame({\n",
    "    'Label': label_cols,\n",
    "    'Min F1 Score': min_f1_scores,\n",
    "    'Max F1 Score': max_f1_scores,\n",
    "    'Average F1 Score': avg_f1_scores\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(f1_stats_df)\n",
    "\n",
    "f1_stats_df.to_csv('/path/to/f1_stats.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3366b73",
   "metadata": {},
   "source": [
    "### Report AUROC, AUPRC, and F1 for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e15911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "f1_scores_per_label = []\n",
    "auroc_scores_per_label = []\n",
    "auprc_scores_per_label = []\n",
    "\n",
    "# Load the logits and calculate predictions\n",
    "for fold, logits_path in enumerate(logits_paths, start=1):\n",
    "    logits = np.loadtxt(logits_path, delimiter=\",\")\n",
    "    probabilities = sigmoid(logits)\n",
    "    predictions = (probabilities > 0.11).astype(int)\n",
    "    \n",
    "    # Load true labels for the current fold\n",
    "    true_labels_path = f'/path/to/true_labels_fold_{fold}.csv'\n",
    "    true_labels = np.loadtxt(true_labels_path, delimiter=\",\")\n",
    "    \n",
    "    # Calculate F1 score for each label\n",
    "    f1_scores = f1_score(true_labels, predictions, average=None)\n",
    "    f1_scores_per_label.append(f1_scores)\n",
    "    \n",
    "    # Calculate AUROC and AUPRC for each label\n",
    "    auroc_scores = [roc_auc_score(true_labels[:, i], probabilities[:, i]) for i in range(len(label_cols))]\n",
    "    auprc_scores = [average_precision_score(true_labels[:, i], probabilities[:, i]) for i in range(len(label_cols))]\n",
    "    \n",
    "    auroc_scores_per_label.append(auroc_scores)\n",
    "    auprc_scores_per_label.append(auprc_scores)\n",
    "\n",
    "# Create DataFrames to store F1, AUROC, and AUPRC scores\n",
    "f1_scores_df = pd.DataFrame(f1_scores_per_label, columns=label_cols)\n",
    "auroc_scores_df = pd.DataFrame(auroc_scores_per_label, columns=label_cols)\n",
    "auprc_scores_df = pd.DataFrame(auprc_scores_per_label, columns=label_cols)\n",
    "\n",
    "# Calculate average scores across all folds\n",
    "avg_f1_scores = f1_scores_df.mean()\n",
    "avg_auroc_scores = auroc_scores_df.mean()\n",
    "avg_auprc_scores = auprc_scores_df.mean()\n",
    "\n",
    "# Create a DataFrame to store the average scores\n",
    "avg_scores_df = pd.DataFrame({\n",
    "    'Label': label_cols,\n",
    "    'Average F1 Score': avg_f1_scores,\n",
    "    'Average AUROC': avg_auroc_scores,\n",
    "    'Average AUPRC': avg_auprc_scores\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(avg_scores_df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "avg_scores_df.to_csv('/path/to/avg_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_note_v2",
   "language": "python",
   "name": "op_note_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
