{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for measuring the accuracy of the model on the human-annotated CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = pd.read_csv('path/to/llama_output.csv')\n",
    "ground_truth = pd.read_csv('path/to/op_note_sampe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to map column in ground truth to column in model output\n",
    "ground_truth_columns = {\n",
    "    'Active_bleeding_from_named_vessel': 'Active bleeding from named vessel',\n",
    "    'Active_bleeding_from_solid_organ': 'Active bleeding from solid organ',\n",
    "    'Class_I': 'Class I',\n",
    "    'Class_II': 'Class II',\n",
    "    'Class_III': 'Class III',\n",
    "    'Class_IV': 'Class IV',\n",
    "    'Bowel_resection': 'Bowel resection',\n",
    "    'Colostomy': 'Colostomy',\n",
    "    'Ileostomy': 'Ileostomy',\n",
    "    'Hand-Sewn_Anastomosis': 'Hand-Sewn Anastomosis',\n",
    "    'Serosal_tear_repair': 'Serosal tear repair',\n",
    "    'Primary_Repair': 'Primary Repair',\n",
    "    'Stapled_Anastomosis': 'Stapled Anastomosis',\n",
    "    'Fascia_Closed_(Interrupted)': 'Fascia Closed (Interrupted)',\n",
    "    'Fascia_Closed_(Running/Continuous)': 'Fascia Closed (Running/Continuous)',\n",
    "    'Fascia_Left_Open': 'Fascia Left Open',\n",
    "    'Skin_Closed_(Full w/ Prevena)': 'Skin Closed (Full w/ Prevena)',\n",
    "    'Skin_Closed_(Full)': 'Skin Closed (Full)',\n",
    "    'Skin_Closed_(Partial)': 'Skin Closed (Partial)',\n",
    "    'Skin_Left_Open': 'Skin Left Open',\n",
    "    'Synthetic': 'Synthetic Mesh'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "\n",
    "# This will be a list of dictionaries with correct_answer, model_prediction, and prediction_explanation\n",
    "wrong_answer_explanations = []\n",
    "\n",
    "pred = {}\n",
    "for column in ground_truth_columns:\n",
    "    pred[f\"{column}\"] = {}\n",
    "    pred[f\"{column}\"][\"tp\"] = 0\n",
    "    pred[f\"{column}\"][\"tn\"] = 0\n",
    "    pred[f\"{column}\"][\"fp\"] = 0\n",
    "    pred[f\"{column}\"][\"fn\"] = 0\n",
    "    pred[f\"{column}\"][\"dk\"] = 0     # \"Don't know\"; this is where the model output is \"-1\"\n",
    "\n",
    "# Simpler version ignoring the skin closure tasks\n",
    "for i in range(len(ground_truth)):\n",
    "    # Assert that the op_note_id is the same for both dataframes\n",
    "    assert ground_truth.at[i, \"ID\"] == model_output.at[i, \"op_note_id\"], f\"i: {i}, ground_truth.at[i, 'ID']: {ground_truth.at[i, 'ID']}, model_output.at[i, 'op_note_id']: {model_output.at[i, 'op_note_id']}\"\n",
    "    for column in ground_truth_columns:\n",
    "        print(f\"i: {i}, column: {column}\")\n",
    "        if ground_truth.at[i, column] == model_output.at[i, ground_truth_columns[column]]:\n",
    "            if ground_truth.at[i, column] == 1:\n",
    "                pred[f\"{column}\"][\"tp\"] += 1\n",
    "            else:\n",
    "                pred[f\"{column}\"][\"tn\"] += 1\n",
    "        elif model_output.at[i, ground_truth_columns[column]] in {0,1}:\n",
    "            wrong_answer_explanations.append({\n",
    "                \"patient_id\": ground_truth.at[i, \"ID\"],\n",
    "                \"task\": column,\n",
    "                \"correct_answer\": ground_truth.at[i, column],\n",
    "                \"model_prediction\": model_output.at[i, ground_truth_columns[column]],\n",
    "                \"prediction_explanation\": model_output.at[i, ground_truth_columns[column] + \"_explanation\"],\n",
    "                \"op_note\": ground_truth.at[i, \"Text_desc\"]\n",
    "            })\n",
    "            if ground_truth.at[i, column] == 1:\n",
    "                pred[f\"{column}\"][\"fn\"] += 1\n",
    "            else:\n",
    "                pred[f\"{column}\"][\"fp\"] += 1\n",
    "        else:\n",
    "            wrong_answer_explanations.append({\n",
    "                \"patient_id\": ground_truth.at[i, \"ID\"],\n",
    "                \"task\": column,\n",
    "                \"correct_answer\": ground_truth.at[i, column],\n",
    "                \"model_prediction\": model_output.at[i, ground_truth_columns[column]],\n",
    "                \"prediction_explanation\": model_output.at[i, ground_truth_columns[column] + \"_explanation\"],\n",
    "                \"op_note\": ground_truth.at[i, \"Text_desc\"]\n",
    "            })\n",
    "            pred[f\"{column}\"][\"dk\"] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skin closure version\n",
    "for i in range(len(ground_truth)):\n",
    "    for column in ground_truth_columns:\n",
    "        if column == \"Skin_Closed_(Full w/ Prevena)\" or column == \"Skin_Closed_(Full)\" or column == \"Skin_Closed_(Partial)\":\n",
    "            if column == \"Skin_Closed_(Full w/ Prevena)\":\n",
    "                if ground_truth.at[i, column] == \"1\" and model_output.at[i, \"extract_skin_closure\"] == 2:\n",
    "                    pred[f\"{column}\"][\"tp\"] += 1\n",
    "            elif column == \"Skin_Closed_(Full)\":\n",
    "                if ground_truth.at[i, column] == \"1\" and model_output.at[i, \"extract_skin_closure\"] == 2:\n",
    "                    pred[f\"{column}\"][\"tp\"] += 1\n",
    "            elif column == \"Skin_Closed_(Partial)\":\n",
    "                if ground_truth.at[i, column] == \"1\" and model_output.at[i, \"extract_skin_closure\"] == 1:\n",
    "                    correct_predictions += 1\n",
    "        elif ground_truth.at[i, column] == model_output.at[i, ground_truth_columns[column]]:\n",
    "            if ground_truth.at[i, column] == 1:\n",
    "                pred[f\"{column}\"][\"tp\"] += 1\n",
    "            else:\n",
    "                pred[f\"{column}\"][\"tn\"] += 1\n",
    "        elif model_output.at[i, ground_truth_columns[column]] in {0,1}:\n",
    "            wrong_answer_explanations.append({\n",
    "                \"patient_id\": ground_truth.at[i, \"ID\"],\n",
    "                \"task\": column,\n",
    "                \"correct_answer\": ground_truth.at[i, column],\n",
    "                \"model_prediction\": model_output.at[i, ground_truth_columns[column]],\n",
    "                \"prediction_explanation\": model_output.at[i, ground_truth_columns[column] + \"_explanation\"],\n",
    "                \"op_note\": ground_truth.at[i, \"Text_desc\"]\n",
    "            })\n",
    "            if ground_truth.at[i, column] == 1:\n",
    "                pred[f\"{column}\"][\"fn\"] += 1\n",
    "            else:\n",
    "                pred[f\"{column}\"][\"fp\"] += 1\n",
    "        else:\n",
    "            wrong_answer_explanations.append({\n",
    "                \"patient_id\": ground_truth.at[i, \"ID\"],\n",
    "                \"task\": column,\n",
    "                \"correct_answer\": ground_truth.at[i, column],\n",
    "                \"model_prediction\": model_output.at[i, ground_truth_columns[column]],\n",
    "                \"prediction_explanation\": model_output.at[i, ground_truth_columns[column] + \"_explanation\"],\n",
    "                \"op_note\": ground_truth.at[i, \"Text_desc\"]\n",
    "            })\n",
    "            pred[f\"{column}\"][\"dk\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create F1 Score for each task using pred dictionary\n",
    "f1_scores = {}\n",
    "for task in pred:\n",
    "    tp = pred[task][\"tp\"]\n",
    "    tn = pred[task][\"tn\"]\n",
    "    fp = pred[task][\"fp\"]\n",
    "    fn = pred[task][\"fn\"]\n",
    "    dk = pred[task][\"dk\"]\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "    pred[task][\"precision\"] = precision\n",
    "    pred[task][\"recall\"] = recall\n",
    "    pred[task][\"f1\"] = f1\n",
    "    f1_scores[task] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred)\n",
    "df.transpose()\n",
    "# df.to_csv('experiments/folds/table5.csv', index=True)\n",
    "df.to_csv('path/to/table.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output pred and f1_scores to a txt file\n",
    "with open('experiments/2024-10-14_8b/results.txt', 'a') as f:\n",
    "# with open('experiments/folds/results1.txt', 'a') as f:\n",
    "    for task in pred:\n",
    "        f.write(f\"Task: {task}\\n\")\n",
    "        f.write(f\"True Positives: {pred[task]['tp']}\\n\")\n",
    "        f.write(f\"True Negatives: {pred[task]['tn']}\\n\")\n",
    "        f.write(f\"False Positives: {pred[task]['fp']}\\n\")\n",
    "        f.write(f\"False Negatives: {pred[task]['fn']}\\n\")\n",
    "        f.write(f\"Don't Know: {pred[task]['dk']}\\n\")\n",
    "        f.write(f\"F1 Score: {f1_scores[task]}\\n\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output wrong_answer_explanations to a csv file grouped by task\n",
    "wrong_answer_explanations_df = pd.DataFrame(wrong_answer_explanations)\n",
    "wrong_answer_explanations_df = wrong_answer_explanations_df.sort_values(by=['task'])\n",
    "wrong_answer_explanations_df.to_csv(\"path/to/wrong_answer_explanations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the F1 Score for each task with a bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "# order the tasks by F1 Score\n",
    "f1_scores = dict(sorted(f1_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "plt.bar(f1_scores.keys(), f1_scores.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score for each task\")\n",
    "plt.savefig(\"path/to/f1_score_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for each task\n",
    "for task in pred:\n",
    "    cm = confusion_matrix([1 if ground_truth.at[i, task] == 1 else 0 for i in range(len(ground_truth))], [1 if model_output.at[i, ground_truth_columns[task]] == 1 else 0 for i in range(len(ground_truth))])\n",
    "    sns.heatmap(cm, annot=True, fmt='g')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix for {task}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
